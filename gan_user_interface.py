# -*- coding: utf-8 -*-
"""GAN_User_Interface.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6-abqJAtlb0O3h4jLMSz3dE8ktLMyRV

#User Interface for Controlling Features of Generated Faces

Beginning of source code for an app/user interface that will allow user to generate faces and control specific features of the face.
"""

"""##Setup
Setting up correct runtime, cloning stylegan2, and defining necessary funtions for face generation.
"""

# Commented out IPython magic to ensure Python compatibility.
# Run this for Google CoLab (use TensorFlow 1.x)
# %tensorflow_version 1.x

!git clone https://github.com/NVlabs/stylegan2.git

!ls /content/stylegan2/

import sys
sys.path.insert(0, "/content/stylegan2")

import dnnlib

# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.
#
# This work is made available under the Nvidia Source Code License-NC.
# To view a copy of this license, visit
# https://nvlabs.github.io/stylegan2/license.html

import argparse
import numpy as np
import PIL.Image
import dnnlib
import dnnlib.tflib as tflib
import re
import sys

import pretrained_networks

#----------------------------------------------------------------------------

def expand_seed(seeds, vector_size):
  result = []

  for seed in seeds:
    rnd = np.random.RandomState(seed)
    result.append( rnd.randn(1, vector_size) ) 
  return result

def generate_images(Gs, seeds, truncation_psi, prefix):
    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]

    Gs_kwargs = dnnlib.EasyDict()
    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)
    Gs_kwargs.randomize_noise = False
    if truncation_psi is not None:
        Gs_kwargs.truncation_psi = truncation_psi

    for seed_idx, seed in enumerate(seeds):
        print('Generating image for seed %d/%d ...' % (seed_idx, len(seeds)))
        rnd = np.random.RandomState(0)
        tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]
        images = Gs.run(seed, None, **Gs_kwargs) # [minibatch, height, width, channel]
        path = f"/content/{prefix}-{seed_idx+1}.png"
        PIL.Image.fromarray(images[0], 'RGB').save(path)

# sc = dnnlib.SubmitConfig()
# sc.num_gpus = 1
# sc.submit_target = dnnlib.SubmitTarget.LOCAL
# sc.local.do_not_copy_source_files = True

# sc.run_desc = 'generate-images'
network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'

# dnnlib.tflib.init_tf() # ??

print('Loading networks from "%s"...' % network_pkl)
_G, _D, Gs = pretrained_networks.load_networks(network_pkl)
vector_size = Gs.input_shape[1:][0]

"""##Import Latent Directions

Import pre-trained latent directions from https://twitter.com/robertluxemburg/status/1207087801344372736
"""

from pathlib import Path

def get_control_latent_vectors(path):
    files = [x for x in Path(path).iterdir() if str(x).endswith('.npy')]
    latent_vectors = {f.name[:-4]:np.load(f) for f in files}
    return latent_vectors

# only run once
!wget 'https://hostb.org/NCM/stylegan2directions.zip'
!unzip stylegan2directions.zip

latent_controls = get_control_latent_vectors('stylegan2directions') # need to get from somewhere else
len(latent_controls), latent_controls.keys(), latent_controls['age'].shape

"""##Generate One Random Face"""

import cv2 
from google.colab.patches import cv2_imshow
import random

def show_img(latent):
  img = generate_images(Gs, latent, 0.5, "image")

  img = cv2.imread('/content/image-1.png')   
  cv2_imshow(img)
  
def generate_latent(seed=random.randrange(0,2**32 - 1)):

  seed = seed
  vector_size = 512
  original_latent = expand_seed([seed], vector_size)
  
  return original_latent

def generate_random_and_show():
  latent = generate_latent()
  show_img(latent)
  return latent

"""## Manipulate Latent Vector of Original Image"""

def manipulate_vector(original_latent, age=0, gender=0, smile=0, pitch=0, roll=0, 
                      yaw=0, eye_eyebrow_distance=0, eye_distance=0, eye_ratio=0, 
                      eyes_open=0, nose_ratio=0, nose_tip=0, nose_mouth_distance=0, 
                      mouth_ratio=0, mouth_open=0, lip_ratio=0):
  new_latent = original_latent + latent_controls['age']*age + latent_controls['gender']*gender + latent_controls['smile']*smile + latent_controls['pitch']*pitch + \
   latent_controls['roll']*roll + latent_controls['yaw']*yaw + latent_controls['eye_eyebrow_distance']*eye_eyebrow_distance + latent_controls['eye_distance']*eye_distance + latent_controls['eye_ratio']*eye_ratio
  # finish this
  return new_latent

def manipulate_and_show(original_latent, age=0, gender=0):
  new_latent = manipulate_vector(original_latent, age=age, gender=gender)
  show_img(new_latent)
  return new_latent
